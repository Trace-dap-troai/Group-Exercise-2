{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3FHaZidf9+bulZH2gD/QH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trace-dap-troai/Group-Exercise-2/blob/main/Group_Excercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x93AVhAqjToL"
      },
      "outputs": [],
      "source": [
        "#data generation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate dataset (>1000 samples, >30 features)\n",
        "X, y = make_regression(\n",
        "    n_samples=1500,\n",
        "    n_features=35,\n",
        "    n_informative=25,\n",
        "    noise=15,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "data = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
        "data[\"target\"] = y\n",
        "\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce some missing values artificially\n",
        "np.random.seed(42)\n",
        "for col in data.columns[:5]:\n",
        "    data.loc[data.sample(frac=0.02).index, col] = np.nan\n",
        "\n",
        "# Handle missing values using median imputation\n",
        "data = data.fillna(data.median())\n"
      ],
      "metadata": {
        "id": "QCADwUrtj5K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing values were handled using median imputation to preserve the\n",
        "distribution of features and avoid bias caused by extreme values.\n"
      ],
      "metadata": {
        "id": "EGXEWlAwkAlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Scaling\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = data.drop(\"target\", axis=1)\n",
        "y = data[\"target\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "oovGPrXgkCeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization was applied to ensure that all features contribute equally\n",
        "to the regression models, especially those sensitive to feature scale such\n",
        "as Ridge and Lasso regression.\n"
      ],
      "metadata": {
        "id": "8VqBBM1vkC7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Feature Selection (SelectKBest)\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=20)\n",
        "X_selected = selector.fit_transform(X_scaled, y)\n",
        "\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "selected_features\n"
      ],
      "metadata": {
        "id": "2bXqGLItkGso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutual Information was used to select the top 20 most informative features.\n",
        "This step reduces dimensionality and improves model interpretability.\n"
      ],
      "metadata": {
        "id": "xhbQi6bDkIzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Building & Evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_selected, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "#Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "r2, rmse\n"
      ],
      "metadata": {
        "id": "__k5AoWYoBpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression was used as a baseline model.\n",
        "RÂ² measures how well the model explains variance, while RMSE quantifies\n",
        "prediction error in the original target scale.\n"
      ],
      "metadata": {
        "id": "w6sF9nwUoXTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#K-Fold Cross Validation\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_r2 = cross_val_score(lr, X_selected, y, cv=kf, scoring=\"r2\")\n",
        "\n",
        "cv_r2.mean(), cv_r2.std()\n"
      ],
      "metadata": {
        "id": "CvxWe1xJoZSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold cross-validation provides a more robust performance estimate by\n",
        "averaging results across multiple data splits.\n"
      ],
      "metadata": {
        "id": "NXoDO6Q-odsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Enhancement\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "ridge = Ridge(alpha=1.0)\n",
        "lasso = Lasso(alpha=0.05)\n",
        "\n",
        "ridge.fit(X_train, y_train)\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "ridge_r2 = r2_score(y_test, ridge.predict(X_test))\n",
        "lasso_r2 = r2_score(y_test, lasso.predict(X_test))\n",
        "\n",
        "ridge_r2, lasso_r2\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "THFoeksAofha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression reduces overfitting by shrinking coefficients,\n",
        "while Lasso regression additionally performs automatic feature selection.\n"
      ],
      "metadata": {
        "id": "d3_EpNejoleY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Polynomial Regression (Creativity)\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "poly_model = Pipeline([\n",
        "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    (\"lr\", LinearRegression())\n",
        "])\n",
        "\n",
        "poly_model.fit(X_train, y_train)\n",
        "poly_pred = poly_model.predict(X_test)\n",
        "\n",
        "poly_r2 = r2_score(y_test, poly_pred)\n",
        "poly_r2\n"
      ],
      "metadata": {
        "id": "oe4MVqXponl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial Regression captures non-linear relationships that cannot be\n",
        "modeled by standard linear regression.\n"
      ],
      "metadata": {
        "id": "nbK0ZNuYosXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "plt.axhline(0, linestyle=\"--\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cGtjnKn4ot1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual analysis helps verify linearity and homoscedasticity assumptions.\n"
      ],
      "metadata": {
        "id": "PGx1lqpIoxxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lasso Feature Importance\n",
        "coef = pd.Series(lasso.coef_, index=selected_features)\n",
        "\n",
        "coef[coef != 0].sort_values().plot(kind=\"barh\", figsize=(6,6))\n",
        "plt.title(\"Lasso Selected Feature Importance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GQZWjBcoozfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso regression highlights the most influential features by driving\n",
        "irrelevant coefficients to zero.\n"
      ],
      "metadata": {
        "id": "LlYsI9Goo3gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONCOLUTION\n",
        "\n",
        "This project demonstrated a complete regression workflow including\n",
        "preprocessing, feature selection, model training, evaluation, and enhancement.\n",
        "\n",
        "Advanced techniques such as regularization and polynomial regression\n",
        "improved performance and provided deeper insights into feature relevance\n",
        "and model behavior.\n"
      ],
      "metadata": {
        "id": "rltXPIcxo63G"
      }
    }
  ]
}